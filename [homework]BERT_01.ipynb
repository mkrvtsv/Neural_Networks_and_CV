{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[homework]BERT_01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkrvtsv/Neural_Networks_and_CV/blob/master/%5Bhomework%5DBERT_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO_wCxS951bT",
        "colab_type": "text"
      },
      "source": [
        "# Homework\n",
        "\n",
        "Привет! В этой домашнем задании ты научишься обучении модели BERT. На семинаре был разобран код модели, здесь же посмотрим на то, как надо обработать данные, чтобы на них модель могла учиться. \n",
        "\n",
        "Замечания по выполнению задания:\n",
        "\n",
        "- Код внутри блока `<DON'T TOUCH THIS!>` используется для проверки задания, его нельзя трогать. \n",
        "\n",
        "- Внутри блока `<YOUR CODE>` может больше кода, чем там показано изначально.\n",
        "\n",
        "- От задания требуется написания небольшого отчета в конце.\n",
        "\n",
        "\n",
        "Для начала загрузи нужные библиотеки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dq4NkRu_u3i",
        "colab_type": "code",
        "outputId": "d914efef-843e-4642-b9a8-de4e497e553a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install transformers catalyst"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.9.0)\n",
            "Requirement already satisfied: catalyst in /usr/local/lib/python3.6/dist-packages (20.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catalyst) (3.2.1)\n",
            "Requirement already satisfied: deprecation in /usr/local/lib/python3.6/dist-packages (from catalyst) (2.1.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from catalyst) (5.5.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from catalyst) (1.5.0+cu101)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from catalyst) (3.13)\n",
            "Requirement already satisfied: tensorboard>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from catalyst) (2.2.1)\n",
            "Requirement already satisfied: GitPython>=3.1.1 in /usr/local/lib/python3.6/dist-packages (from catalyst) (3.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from catalyst) (20.3)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (from catalyst) (2.0)\n",
            "Requirement already satisfied: plotly>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from catalyst) (4.4.1)\n",
            "Requirement already satisfied: pandas>=0.22 in /usr/local/lib/python3.6/dist-packages (from catalyst) (1.0.3)\n",
            "Requirement already satisfied: crc32c>=1.7 in /usr/local/lib/python3.6/dist-packages (from catalyst) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from catalyst) (0.22.2.post1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catalyst) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catalyst) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catalyst) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catalyst) (2.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (46.1.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (2.1.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (4.3.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->catalyst) (1.0.18)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->catalyst) (0.16.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst) (0.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst) (0.34.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst) (1.7.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst) (3.10.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst) (1.28.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst) (0.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14.0->catalyst) (1.6.0.post3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.6/dist-packages (from GitPython>=3.1.1->catalyst) (4.0.5)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly>=4.1.0->catalyst) (1.3.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22->catalyst) (2018.9)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->catalyst) (1.4.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->catalyst) (0.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->catalyst) (0.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->catalyst) (0.1.9)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14.0->catalyst) (1.3.0)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from gitdb<5,>=4.0.1->GitPython>=3.1.1->catalyst) (3.0.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard>=1.14.0->catalyst) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14.0->catalyst) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlHnoGZN6OKy",
        "colab_type": "code",
        "outputId": "497f1398-9d94-419d-d1c7-ee3ec578400b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "import os\n",
        "import random\n",
        "import sys\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, RandomSampler, Dataset\n",
        "\n",
        "import transformers\n",
        "\n",
        "from catalyst.dl import SupervisedRunner\n",
        "from catalyst.dl.callbacks import AccuracyCallback, SchedulerCallback, F1ScoreCallback\n",
        "from catalyst.utils import set_global_seed, prepare_cudnn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning:\n",
            "\n",
            "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning:\n",
            "\n",
            "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning:\n",
            "\n",
            "numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmlqBOSO7a5L",
        "colab_type": "text"
      },
      "source": [
        "Внизу идет технический код, который нужен для загрузки датасетов. Его можно уменьшить, выбрав только некоторые из них. Для того, что бы зачесть задание, надо выбрать не менее двух задач, для хотя бы одной из которых нужно использовать два предложения(ответ и вопрос, два предложения и прочее). Подробнее про датасеты [здесь](https://gluebenchmark.com/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4H6_6zgnfRQR",
        "colab": {}
      },
      "source": [
        "TASKS = [\"CoLA\", \"SST\", \"MRPC\", \"QQP\", \"STS\", \"MNLI\", \"SNLI\", \"QNLI\", \"RTE\", \"WNLI\"]\n",
        "TASK2PATH = {\n",
        "    \"CoLA\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FCoLA.zip?alt=media&token=46d5e637-3411-4188-bc44-5809b5bfb5f4\",\n",
        "    \"SST\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8\",\n",
        "    \"MRPC\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc\",\n",
        "    \"QQP\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQQP.zip?alt=media&token=700c6acf-160d-4d89-81d1-de4191d02cb5\",\n",
        "    \"STS\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSTS-B.zip?alt=media&token=bddb94a7-8706-4e0d-a694-1109e12273b5\",\n",
        "    \"MNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FMNLI.zip?alt=media&token=50329ea1-e339-40e2-809c-10c40afff3ce\",\n",
        "    \"SNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSNLI.zip?alt=media&token=4afcfbb2-ff0c-4b2d-a09a-dbf07926f4df\",\n",
        "    \"QNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQNLIv2.zip?alt=media&token=6fdcf570-0fc5-4631-8456-9505272d1601\",\n",
        "    \"RTE\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb\",\n",
        "    \"WNLI\": \"https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FWNLI.zip?alt=media&token=068ad0a0-ded7-4bd7-99a5-5e00222e0faf\",\n",
        "}\n",
        "\n",
        "MRPC_TRAIN = \"https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\"\n",
        "MRPC_TEST = \"https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt\"\n",
        "\n",
        "data_dir = \"data/\"\n",
        "max_seq_length = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O8Y-go7czun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_and_extract(task, data_dir):\n",
        "    print(\"Downloading and extracting %s...\" % task)\n",
        "    data_file = \"%s.zip\" % task\n",
        "    urllib.request.urlretrieve(TASK2PATH[task], data_file)\n",
        "    with zipfile.ZipFile(data_file) as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "    os.remove(data_file)\n",
        "    print(\"\\tCompleted!\")\n",
        "\n",
        "def format_mrpc(data_dir, path_to_data):\n",
        "    print(\"Processing MRPC...\")\n",
        "    mrpc_dir = os.path.join(data_dir, \"MRPC\")\n",
        "    if not os.path.isdir(mrpc_dir):\n",
        "        os.mkdir(mrpc_dir)\n",
        "    if path_to_data:\n",
        "        mrpc_train_file = os.path.join(path_to_data, \"msr_paraphrase_train.txt\")\n",
        "        mrpc_test_file = os.path.join(path_to_data, \"msr_paraphrase_test.txt\")\n",
        "    else:\n",
        "        print(\"Local MRPC data not specified, downloading data from %s\" % MRPC_TRAIN)\n",
        "        mrpc_train_file = os.path.join(mrpc_dir, \"msr_paraphrase_train.txt\")\n",
        "        mrpc_test_file = os.path.join(mrpc_dir, \"msr_paraphrase_test.txt\")\n",
        "        urllib.request.urlretrieve(MRPC_TRAIN, mrpc_train_file)\n",
        "        urllib.request.urlretrieve(MRPC_TEST, mrpc_test_file)\n",
        "    assert os.path.isfile(mrpc_train_file), \"Train data not found at %s\" % mrpc_train_file\n",
        "    assert os.path.isfile(mrpc_test_file), \"Test data not found at %s\" % mrpc_test_file\n",
        "    urllib.request.urlretrieve(TASK2PATH[\"MRPC\"], os.path.join(mrpc_dir, \"dev_ids.tsv\"))\n",
        "\n",
        "    dev_ids = []\n",
        "    with open(os.path.join(mrpc_dir, \"dev_ids.tsv\"), encoding=\"utf8\") as ids_fh:\n",
        "        for row in ids_fh:\n",
        "            dev_ids.append(row.strip().split(\"\\t\"))\n",
        "\n",
        "    with open(mrpc_train_file, encoding=\"utf8\") as data_fh, open(\n",
        "        os.path.join(mrpc_dir, \"train.tsv\"), \"w\", encoding=\"utf8\"\n",
        "    ) as train_fh, open(os.path.join(mrpc_dir, \"dev.tsv\"), \"w\", encoding=\"utf8\") as dev_fh:\n",
        "        header = data_fh.readline()\n",
        "        train_fh.write(header)\n",
        "        dev_fh.write(header)\n",
        "        for row in data_fh:\n",
        "            label, id1, id2, s1, s2 = row.strip().split(\"\\t\")\n",
        "            if [id1, id2] in dev_ids:\n",
        "                dev_fh.write(\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (label, id1, id2, s1, s2))\n",
        "            else:\n",
        "                train_fh.write(\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (label, id1, id2, s1, s2))\n",
        "\n",
        "    with open(mrpc_test_file, encoding=\"utf8\") as data_fh, open(\n",
        "        os.path.join(mrpc_dir, \"test.tsv\"), \"w\", encoding=\"utf8\"\n",
        "    ) as test_fh:\n",
        "        header = data_fh.readline()\n",
        "        test_fh.write(\"index\\t#1 ID\\t#2 ID\\t#1 String\\t#2 String\\n\")\n",
        "        for idx, row in enumerate(data_fh):\n",
        "            label, id1, id2, s1, s2 = row.strip().split(\"\\t\")\n",
        "            test_fh.write(\"%d\\t%s\\t%s\\t%s\\t%s\\n\" % (idx, id1, id2, s1, s2))\n",
        "    print(\"\\tCompleted!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4hNWuZ5okuj",
        "colab_type": "code",
        "outputId": "db78b033-e63b-409e-c772-393e127f65ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "TASKS = ['RTE'] # Или можно просто сюда вписать те датасеты, которые ты выбрал.\n",
        "\n",
        "for task in TASKS:\n",
        "    if task == \"MRPC\":\n",
        "        format_mrpc(data_dir, None)\n",
        "    else:\n",
        "        download_and_extract(task, data_dir)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading and extracting RTE...\n",
            "\tCompleted!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN_uDaDmDL1-",
        "colab_type": "code",
        "outputId": "556f5098-cceb-44c9-f5ed-a78bbff7bc6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls ./data/RTE"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev.tsv  test.tsv  train.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5S3iSoK8LOG",
        "colab_type": "text"
      },
      "source": [
        "Загрузи один из выбранных датасет с помощью Pandas(не обязательно через него, но так проще) и посмотри на него."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBn6ejxaokw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Вместо test-а возьмите valid, а valid сделай из train.\n",
        "# '''\n",
        "# Для RTE есть нормальный valid и test соответственно dev.tsv и test.tsv,\n",
        "# поэтому пока использую их и train не разбиваю, что сделать не сложно.\n",
        "# '''\n",
        "\n",
        "# # <YOUR CODE>\n",
        "# train_pd = pd.read_csv('./data/RTE/train.tsv', sep='\\t', index_col='index')\n",
        "# test_pd = pd.read_csv('./data/RTE/test.tsv', sep='\\t', index_col='index')\n",
        "\n",
        "# mask = np.random.rand(len(train_pd)) < 0.9\n",
        "# valid_pd = train_pd[~mask]\n",
        "# train_pd = train_pd[mask]\n",
        "# print(train_pd.shape, valid_pd.shape, test_pd.shape)\n",
        "# # </YOUR CODE> (2490, 3) (277, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yfQJax21ety",
        "colab_type": "code",
        "outputId": "c0840951-7213-43e1-e436-2d1ca2c624e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Вместо test-а возьмите valid, а valid сделай из train.\n",
        "'''\n",
        "Для RTE есть нормальный valid и test соответственно dev.tsv и test.tsv,\n",
        "поэтому пока использую их и train не разбиваю, что сделать не сложно.\n",
        "'''\n",
        "\n",
        "# <YOUR CODE>\n",
        "train_pd = pd.read_csv('./data/RTE/train.tsv', sep='\\t', index_col='index')\n",
        "test_pd = pd.read_csv('./data/RTE/dev.tsv', sep='\\t', index_col='index')\n",
        "\n",
        "mask = np.random.rand(len(train_pd)) < 0.9\n",
        "valid_pd = train_pd[~mask]\n",
        "train_pd = train_pd[mask]\n",
        "print(f'train: {train_pd.shape}, valid: {valid_pd.shape}, test: {test_pd.shape}')\n",
        "# </YOUR CODE> (2490, 3) (277, 3)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: (2218, 3), valid: (272, 3), test: (277, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWPSpI7AlzIS",
        "colab_type": "code",
        "outputId": "2866e77b-f60c-4ed3-a8f2-3ba59e24dda6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "train_pd.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>No Weapons of Mass Destruction Found in Iraq Yet.</td>\n",
              "      <td>Weapons of Mass Destruction Found in Iraq.</td>\n",
              "      <td>not_entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A place of sorrow, after Pope John Paul II die...</td>\n",
              "      <td>Pope Benedict XVI is the new leader of the Rom...</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Herceptin was already approved to treat the si...</td>\n",
              "      <td>Herceptin can be used to treat breast cancer.</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Judie Vivian, chief executive at ProMedica, a ...</td>\n",
              "      <td>The previous name of Ho Chi Minh City was Saigon.</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A man is due in court later charged with the m...</td>\n",
              "      <td>Paul Stewart Hutchinson is accused of having s...</td>\n",
              "      <td>not_entailment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence1  ...           label\n",
              "index                                                     ...                \n",
              "0      No Weapons of Mass Destruction Found in Iraq Yet.  ...  not_entailment\n",
              "1      A place of sorrow, after Pope John Paul II die...  ...      entailment\n",
              "2      Herceptin was already approved to treat the si...  ...      entailment\n",
              "3      Judie Vivian, chief executive at ProMedica, a ...  ...      entailment\n",
              "4      A man is due in court later charged with the m...  ...  not_entailment\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5o20E_NYmLX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ca345d2-42e1-44c3-ccaa-a9e6828cef21"
      },
      "source": [
        "# train_pd.apply(lambda row: tokenizer.encode(row['sentence1'], row['sentence2']), axis=1).head()\n",
        "# train_pd.apply(lambda row: tokenizer.encode(row['sentence1'], row['sentence2'], max_length=max_seq_length, pad_to_max_length=True)['attention_mask'],\n",
        "#                                               axis=1)\n",
        "# encoded_plus = tokenizer.encode(train_pd['sentence1'][1], text_pair=train_pd['sentence2'][1],\n",
        "#                                 max_length=max_seq_length, pad_to_max_length=True)\n",
        "# tokenizer.encode_plus(train_pd['sentence1'][1],\n",
        "#                                      text_pair=train_pd['sentence2'][1],\n",
        "#                                      max_length=max_seq_length,\n",
        "#                                      pad_to_max_length=True)['attention_mask']\n",
        "\n",
        "# train_pd.apply(lambda row: tokenizer.encode_plus(row['sentence1'],\n",
        "#                                      text_pair=row['sentence2'],\n",
        "#                                      max_length=max_seq_length,\n",
        "#                                      pad_to_max_length=True)['attention_mask'], axis=1).values\n",
        "train_pd.label.apply(lambda e: 0 if e == 'entailment' else 1).values"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 1, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-1xsyToKkP3",
        "colab_type": "code",
        "outputId": "b660b869-343f-43e0-c799-e82a72a2043c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "train_pd.iloc[0, 0], train_pd.iloc[0, 1]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('No Weapons of Mass Destruction Found in Iraq Yet.',\n",
              " 'Weapons of Mass Destruction Found in Iraq.')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or9kgtAEl4eX",
        "colab_type": "code",
        "outputId": "7c7e4c17-31f9-494d-85d3-24a0ce714a4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "train_pd.label.value_counts()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "entailment        1123\n",
              "not_entailment    1094\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK5iar5f8e_7",
        "colab_type": "text"
      },
      "source": [
        "Для начала рассмотрим важную часть обработки текста для трансфомера (и не только) – токенайзер.\n",
        "\n",
        "В качестве примера токенайзера воспользуемся внутренним из библиотеки transformers, обученным для BERT-а. Посмотрим, что он умеет."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I3h53-DpofT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = 'bert-base-uncased'\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0BjZAx68v6T",
        "colab_type": "text"
      },
      "source": [
        "Посмотрим, как происходит токенизация предложения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDkvs3uQpsCY",
        "colab_type": "code",
        "outputId": "90b600b5-53c5-4917-820b-4335d64d83c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_sentence = \"Hide new secretions from the parental units.\"\n",
        "print(tokenizer.tokenize(test_sentence))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hide', 'new', 'secret', '##ions', 'from', 'the', 'parental', 'units', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La_1q8Cj82e8",
        "colab_type": "text"
      },
      "source": [
        "Видно, что предложения разделяются не на слова, а подслова. Токены, которые надо объеденить в слова для получения \"нормального\" текста, выделены с помощью `##`. Посмотрим, как различаются коды токенов с этим символом и без него."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKd0SwME9bi3",
        "colab_type": "code",
        "outputId": "40bbc5e6-10e4-4eb6-98da-28c0d68203a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(tokenizer.convert_tokens_to_ids(['ions']))\n",
        "print(tokenizer.convert_tokens_to_ids(['##ions']))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[15956]\n",
            "[8496]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5qZXD5Z-D5o",
        "colab_type": "text"
      },
      "source": [
        "Для токенизации предложений воспользуемся методом `encode`. Он принимает предложение как строку или список токенов**(!)**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUZQ6tMNpsEh",
        "colab_type": "code",
        "outputId": "410ff3a5-c8a2-42bd-d1a8-d07b1a635067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tokenizer.encode(test_sentence))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[101, 5342, 2047, 3595, 8496, 2013, 1996, 18643, 3197, 1012, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiVCb6RG-Yi2",
        "colab_type": "text"
      },
      "source": [
        "Добавились специальные токены впереди и сзади предложения. Посмотрим на весь список специальных токенов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AqoVIrktk4v",
        "colab_type": "code",
        "outputId": "bf21d37d-e97e-4d00-83fd-868e59bb09d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(tokenizer.special_tokens_map)\n",
        "print({i: j for i, j in zip(tokenizer.all_special_tokens, tokenizer.all_special_ids)})"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
            "{'[SEP]': 102, '[CLS]': 101, '[MASK]': 103, '[UNK]': 100, '[PAD]': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TA4XbeA-phM",
        "colab_type": "text"
      },
      "source": [
        "Посмотрим, что ещё может делать токенайзер. Что требуется нам для обучения BERT-а: добавить паддинг, получить маску аттеншена и тип токенов. Попробуем сделать это самостоятельно и посмотрим, как это сделать с помощью токенайзера.\n",
        "\n",
        "Выбери два предложения из обучающей выборки. Получи их токены с помощью метода `tokenize`. Объедени списки токенов так, чтобы модель могла различать, что они от разных предложений. \n",
        "\n",
        "(Подсказка: на семинаре была картинка с эмбеддингами. Она может подсказать, что надо изменить в токенах предложения) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4esxYXTipsG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <YOUR CODE>\n",
        "s1, s2 = train_pd.loc[383, 'sentence1'], train_pd.loc[383, 'sentence2']\n",
        "tokenized_s1, tokenized_s2 = tokenizer.tokenize(s1), tokenizer.tokenize(s2)\n",
        "s_union = tokenized_s1 + ['[SEP]'] + tokenized_s2\n",
        "# </YOUR CODE>\n",
        "\n",
        "# <DON'T TOUCH THIS!>\n",
        "assert tokenizer.encode(s_union) == tokenizer.encode(s1, s2), \"Not equal\"\n",
        "# </DON'T TOUCH THIS!>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k6nrFRHBJBl",
        "colab_type": "text"
      },
      "source": [
        "Теперь надо добавь нулей в полученный список чисел, чтобы они легко складывались в батчи."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY-xd0_qp9rJ",
        "colab_type": "code",
        "outputId": "bd2867d4-5fe1-495e-b538-7e6d3821c562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# <YOUR CODE>\n",
        "encoded_full = tokenizer.encode(s_union)\n",
        "encoded_full = encoded_full + [0] * (max_seq_length - len(encoded_full))\n",
        "\n",
        "print(encoded_full[-20:])\n",
        "# </YOUR CODE>\n",
        "\n",
        "# <DON'T TOUCH THIS!>\n",
        "encoded_correct = tokenizer.encode(s1, s2, max_length=max_seq_length, pad_to_max_length=True)\n",
        "assert len(encoded_full) == len(encoded_correct), \"Different length\"\n",
        "assert encoded_full == encoded_correct, \"Not equal\"\n",
        "# </DON'T TOUCH THIS!>"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6017, 2386, 1012, 102, 25616, 4880, 7174, 2003, 1996, 2873, 1997, 1996, 2394, 2136, 1012, 102, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd5yNVukBZuC",
        "colab_type": "text"
      },
      "source": [
        "В модель также надо кинуть маску для механизма внимания и тип предложения для каждого токена. Сделай их."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hiljf3_vQ75",
        "colab_type": "code",
        "outputId": "e1c0edef-33fe-4037-c5e9-eb30a48b5632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# <YOUR CODE>\n",
        "token_type_ids = [0 for e in encoded_full[: encoded_full.index(102)+1]] +\\\n",
        "                 [e > 0 for e in encoded_full[encoded_full.index(102)+1 :]]\n",
        "attention_mask = [e > 0 for e in encoded_full]\n",
        "\n",
        "print(list(map(int, token_type_ids[-20:])))\n",
        "print(list(map(int, attention_mask[-20:])))\n",
        "# </YOUR CODE>\n",
        "\n",
        "# <DON'T TOUCH THIS!>\n",
        "encoded_plus = tokenizer.encode_plus(s1, text_pair=s2, max_length=max_seq_length, pad_to_max_length=True)\n",
        "assert len(token_type_ids) == len(encoded_plus['token_type_ids']), \"Different length in token_type_ids\"\n",
        "assert token_type_ids == encoded_plus['token_type_ids'], \"Not equal token_type_ids\"\n",
        "assert len(attention_mask) == len(encoded_plus['attention_mask']), \"Different length in attention_mask\"\n",
        "assert attention_mask == encoded_plus['attention_mask'], \"Not equal attention_mask\"\n",
        "# </DON'T TOUCH THIS!>"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxruIU08CEgg",
        "colab_type": "text"
      },
      "source": [
        "Как видно из тестов, все нужные для обработки текста для BERT-а вещи может делать токенизатор из `transformers`. Но не все токенизаторы настолько функциональны. Их (почти)полный список:\n",
        "- [Sentence Piece](https://github.com/google/sentencepiece/)\n",
        "- [fastBPE](https://github.com/glample/fastBPE)\n",
        "- [Hugging Face Tokenizers](https://github.com/huggingface/tokenizers)\n",
        "- [YouTokenToMe](https://github.com/VKCOM/YouTokenToMe)\n",
        "\n",
        "Их сравнивают [здесь](https://github.com/VKCOM/YouTokenToMe/blob/master/benchmark.md) или [здесь](https://towardsdatascience.com/a-small-timing-experiment-on-the-new-tokenizers-library-a-write-up-7caab6f80ea6). Также специальные токенайзеры, которые специализируются на \"незападные\" языки. Но не будем на них останавливаться.\n",
        "\n",
        "Теперь ты знаешь достаточно, чтобы написать обработчик данных. Что надо сделать: получить из данных предложения, закодировать их, получить аттенш маску и тип токенов, не забыть про таргет. \n",
        "\n",
        "P.S. Есть более быстрая версия токенизатора для BERT внутри `transformers`, `BertTokenizerFast`. \n",
        "\n",
        "P.S.S. Теперь надо использовать только функционал токенайзера для кодирования предложений, без велосипедов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y-wgeHHokz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # <YOUR CODE>\n",
        "        self.features = self.data.apply(lambda row:\n",
        "                                        tokenizer.\n",
        "                                        encode(row['sentence1'],\n",
        "                                               row['sentence2'],\n",
        "                                               max_length=max_seq_length,\n",
        "                                               pad_to_max_length=True), axis=1).values\n",
        "\n",
        "        self.attention_mask = self.data.apply(lambda row:\n",
        "                                              tokenizer.\n",
        "                                              encode_plus(row['sentence1'],\n",
        "                                                          text_pair=row['sentence2'],\n",
        "                                                          max_length=max_seq_length,\n",
        "                                                          pad_to_max_length=True)['attention_mask'], axis=1).values\n",
        "\n",
        "        self.token_types_ids = self.data.apply(lambda row:\n",
        "                                               tokenizer.\n",
        "                                               encode_plus(row['sentence1'],\n",
        "                                                           text_pair=row['sentence2'],\n",
        "                                                           max_length=max_seq_length,\n",
        "                                                           pad_to_max_length=True)['token_type_ids'], axis=1).values\n",
        "\n",
        "        self.target = self.data.label.apply(lambda e: 0 if e == 'entailment' else 1).values\n",
        "        # </YOUR CODE>\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)       # return len(features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.features[idx]),                # feature\n",
        "            'attention_mask': torch.tensor(self.attention_mask[idx]),\n",
        "            'token_type_ids': torch.tensor(self.token_types_ids[idx]),    # token_types_ids\n",
        "            'targets': torch.tensor(self.target[idx])\n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFVoCAkiFGJt",
        "colab_type": "text"
      },
      "source": [
        "Воспользуйтесь семинаром и построй модель для классификации предложений.\n",
        "\n",
        "(Подсказка: весь код BERT-а из семинара доступен из библиотеки `transformers`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa6coCLFFSZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertForSequenceClassification(nn.Module):\n",
        "    def __init__(self, pretrained_model_name: str, num_labels: int):\n",
        "        super().__init__()\n",
        "\n",
        "        # <YOUR CODE>\n",
        "        config = transformers.BertConfig.from_pretrained(\n",
        "            pretrained_model_name, \n",
        "            num_labels=num_labels\n",
        "        )\n",
        "\n",
        "        self.bert = transformers.BertModel.from_pretrained(\n",
        "            pretrained_model_name,\n",
        "            config=config\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)    # , num_labels=num_labels\n",
        "        self.dropout = nn.Dropout(self.bert.config.hidden_dropout_prob)\n",
        "        # </YOUR CODE>\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
        "\n",
        "        # <YOUR CODE>\n",
        "        assert attention_mask is not None, \"attention mask is none\"\n",
        "        bert_output = self.bert(input_ids=input_ids,\n",
        "                                attention_mask=attention_mask,\n",
        "                                token_type_ids=token_type_ids)\n",
        "\n",
        "        hidden_state = bert_output[0]  # (bs, seq_len, dim)\n",
        "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
        "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
        "        logits = self.classifier(pooled_output)  # (bs, dim)\n",
        "        # </YOUR CODE>\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI9IMsE0G1gh",
        "colab_type": "text"
      },
      "source": [
        "Выбери из [списка](https://huggingface.co/models?search=google%2Fbert_) несколько моделей, которые ты будешь обучать. Сравни их качество на выбранных датасетах. \n",
        "\n",
        "Лучше всего будет выбрать одну основную конфигурацию, и другие с небольшим изменением. Например, пройтись по такой сетке: `{'layers': [2, 4], 'num_heads': [2, 4]}`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVSVPwi-JDIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "num_labels = 2      # 2\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-22wBwrFMob",
        "colab_type": "code",
        "outputId": "09ac36ae-9021-42dc-c4b8-9dcc1ec2dd6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# <YOUR CODE>\n",
        "pretrained_model_name = 'google/bert_uncased_L-2_H-128_A-2' # \"bert-base-uncased\"\n",
        "# 'google/bert_uncased_L-2_H-128_A-2', 'google/bert_uncased_L-2_H-256_A-4',\n",
        "# 'google/bert_uncased_L-4_H-128_A-2', 'google/bert_uncased_L-4_H-256_A-4\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(pretrained_model_name)\n",
        "model = BertForSequenceClassification(pretrained_model_name, num_labels=num_labels)\n",
        "# model = transformers.AutoModelWithLMHead.from_pretrained(pretrained_model_name)\n",
        "# </YOUR CODE>\n",
        "\n",
        "model.to(device)\n",
        "print(\"Success!\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCL7wstMczw9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c5e8da3-4a5a-418c-fddf-032c8795482d"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "\n",
        "# <YOUR CODE>\n",
        "train_dataset = TextClassificationDataset(train_pd, tokenizer)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "valid_dataset = TextClassificationDataset(valid_pd, tokenizer)\n",
        "valid_sampler = RandomSampler(valid_dataset)\n",
        "valid_dataloader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=batch_size)\n",
        "\n",
        "test_dataset = TextClassificationDataset(test_pd, tokenizer)\n",
        "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "dataloaders = {\n",
        "    \"train\": train_dataloader,\n",
        "    \"valid\": valid_dataloader,\n",
        "    \"test\": test_dataloader\n",
        "}\n",
        "\n",
        "print(f\"Dataset size: {len(dataloaders)}\")\n",
        "# </YOUR CODE>"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size: 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VsyoAmwjnb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 404\n",
        "set_global_seed(seed)\n",
        "prepare_cudnn(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "38ojXge_Hy71",
        "colab": {}
      },
      "source": [
        "# Гиперпараметры для обучения модели. Подбери нужные для каждой модели.\n",
        "\n",
        "epochs = 10\n",
        "lr = 1e-5       # 5e-5       # 1e-5\n",
        "warmup_steps = len(train_dataloader) // 2       # 2\n",
        "\n",
        "t_total = len(train_dataloader) * epochs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3TVwcWzH1ok",
        "colab_type": "text"
      },
      "source": [
        "Добавь Loss, Optimizer и Scheduler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mifqwFYdjnWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer_grouped_parameters = [\n",
        "    {\"params\": [p for n, p in model.named_parameters()], \"weight_decay\": 0.0},     #  \"weight_decay\": 0.0\n",
        "]\n",
        "\n",
        "# <YOUR CODE>\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=lr)\n",
        "scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
        ")\n",
        "# </YOUR CODE>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbbWyPiE7MgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_dir = 'logs/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTXx8X69IClJ",
        "colab_type": "text"
      },
      "source": [
        "Для обучения модели воспользуемся библиотекой `catalyst`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFZ9z53VE5tB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "278a928f-cc1c-414f-e0cb-79a94ca152ac"
      },
      "source": [
        "runner = SupervisedRunner(\n",
        "    input_key=(\n",
        "        \"input_ids\",\n",
        "        \"attention_mask\",\n",
        "        \"token_type_ids\"\n",
        "    ),\n",
        "    input_target_key='targets'\n",
        ")\n",
        "\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    loaders=dataloaders,        #train_val_loaders,\n",
        "    callbacks=[\n",
        "        AccuracyCallback(num_classes=num_labels),\n",
        "        SchedulerCallback(mode='batch'),\n",
        "    ],\n",
        "    logdir=log_dir,\n",
        "    num_epochs=epochs,\n",
        "    verbose=True,\n",
        ")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/10 * Epoch (train):   6% 4/70 [00:00<00:02, 22.74it/s, accuracy01=0.625, loss=0.609, lr=1.429e-06, momentum=0.900]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning:\n",
            "\n",
            "To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1/10 * Epoch (train): 100% 70/70 [00:02<00:00, 27.93it/s, accuracy01=0.700, loss=0.663, lr=9.474e-06, momentum=0.900]\n",
            "1/10 * Epoch (valid): 100% 9/9 [00:00<00:00, 73.78it/s, accuracy01=0.250, loss=0.794]\n",
            "1/10 * Epoch (test): 100% 9/9 [00:00<00:00, 75.15it/s, accuracy01=0.524, loss=0.707]\n",
            "[2020-05-11 12:49:04,679] \n",
            "1/10 * Epoch 1 (_base): lr=9.474e-06 | momentum=0.9000\n",
            "1/10 * Epoch 1 (train): accuracy01=0.4899 | loss=0.7495 | lr=7.436e-06 | momentum=0.9000\n",
            "1/10 * Epoch 1 (valid): accuracy01=0.5312 | loss=0.7028\n",
            "1/10 * Epoch 1 (test): accuracy01=0.5270 | loss=0.7085\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning:\n",
            "\n",
            "Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2/10 * Epoch (train): 100% 70/70 [00:02<00:00, 28.38it/s, accuracy01=0.500, loss=0.651, lr=8.421e-06, momentum=0.900]\n",
            "2/10 * Epoch (valid): 100% 9/9 [00:00<00:00, 78.31it/s, accuracy01=0.375, loss=0.724]\n",
            "2/10 * Epoch (test): 100% 9/9 [00:00<00:00, 80.00it/s, accuracy01=0.524, loss=0.694]\n",
            "[2020-05-11 12:49:08,446] \n",
            "2/10 * Epoch 2 (_base): lr=8.421e-06 | momentum=0.9000\n",
            "2/10 * Epoch 2 (train): accuracy01=0.5089 | loss=0.7265 | lr=8.940e-06 | momentum=0.9000\n",
            "2/10 * Epoch 2 (valid): accuracy01=0.5312 | loss=0.6979\n",
            "2/10 * Epoch 2 (test): accuracy01=0.5304 | loss=0.7005\n",
            "3/10 * Epoch (train): 100% 70/70 [00:02<00:00, 28.43it/s, accuracy01=0.200, loss=0.868, lr=7.368e-06, momentum=0.900]\n",
            "3/10 * Epoch (valid): 100% 9/9 [00:00<00:00, 75.38it/s, accuracy01=0.562, loss=0.674]\n",
            "3/10 * Epoch (test): 100% 9/9 [00:00<00:00, 75.70it/s, accuracy01=0.571, loss=0.688]\n",
            "[2020-05-11 12:49:12,468] \n",
            "3/10 * Epoch 3 (_base): lr=7.368e-06 | momentum=0.9000\n",
            "3/10 * Epoch 3 (train): accuracy01=0.5229 | loss=0.7183 | lr=7.887e-06 | momentum=0.9000\n",
            "3/10 * Epoch 3 (valid): accuracy01=0.5382 | loss=0.6927\n",
            "3/10 * Epoch 3 (test): accuracy01=0.5357 | loss=0.6966\n",
            "4/10 * Epoch (train): 100% 70/70 [00:02<00:00, 28.45it/s, accuracy01=0.600, loss=0.711, lr=6.316e-06, momentum=0.900]\n",
            "4/10 * Epoch (valid): 100% 9/9 [00:00<00:00, 73.34it/s, accuracy01=0.688, loss=0.638]\n",
            "4/10 * Epoch (test): 100% 9/9 [00:00<00:00, 77.29it/s, accuracy01=0.571, loss=0.685]\n",
            "[2020-05-11 12:49:16,523] \n",
            "4/10 * Epoch 4 (_base): lr=6.316e-06 | momentum=0.9000\n",
            "4/10 * Epoch 4 (train): accuracy01=0.5371 | loss=0.7117 | lr=6.835e-06 | momentum=0.9000\n",
            "4/10 * Epoch 4 (valid): accuracy01=0.5729 | loss=0.6878\n",
            "4/10 * Epoch 4 (test): accuracy01=0.5357 | loss=0.6948\n",
            "5/10 * Epoch (train): 100% 70/70 [00:02<00:00, 28.52it/s, accuracy01=0.600, loss=0.667, lr=5.263e-06, momentum=0.900]\n",
            "5/10 * Epoch (valid): 100% 9/9 [00:00<00:00, 73.63it/s, accuracy01=0.688, loss=0.664]\n",
            "5/10 * Epoch (test): 100% 9/9 [00:00<00:00, 72.56it/s, accuracy01=0.571, loss=0.680]\n",
            "[2020-05-11 12:49:20,606] \n",
            "5/10 * Epoch 5 (_base): lr=5.263e-06 | momentum=0.9000\n",
            "5/10 * Epoch 5 (train): accuracy01=0.5260 | loss=0.7105 | lr=5.782e-06 | momentum=0.9000\n",
            "5/10 * Epoch 5 (valid): accuracy01=0.5556 | loss=0.6910\n",
            "5/10 * Epoch 5 (test): accuracy01=0.5392 | loss=0.6917\n",
            "6/10 * Epoch (train): 100% 70/70 [00:02<00:00, 28.66it/s, accuracy01=0.400, loss=0.839, lr=4.211e-06, momentum=0.900]\n",
            "6/10 * Epoch (valid): 100% 9/9 [00:00<00:00, 79.26it/s, accuracy01=0.562, loss=0.662]\n",
            "6/10 * Epoch (test): 100% 9/9 [00:00<00:00, 76.60it/s, accuracy01=0.571, loss=0.677]\n",
            "[2020-05-11 12:49:23,502] \n",
            "6/10 * Epoch 6 (_base): lr=4.211e-06 | momentum=0.9000\n",
            "6/10 * Epoch 6 (train): accuracy01=0.5477 | loss=0.6997 | lr=4.729e-06 | momentum=0.9000\n",
            "6/10 * Epoch 6 (valid): accuracy01=0.5556 | loss=0.6930\n",
            "6/10 * Epoch 6 (test): accuracy01=0.5531 | loss=0.6903\n",
            "7/10 * Epoch (train): 100% 70/70 [00:02<00:00, 28.46it/s, accuracy01=0.700, loss=0.644, lr=3.158e-06, momentum=0.900]\n",
            "7/10 * Epoch (valid): 100% 9/9 [00:00<00:00, 77.18it/s, accuracy01=0.625, loss=0.732]\n",
            "7/10 * Epoch (test): 100% 9/9 [00:00<00:00, 76.63it/s, accuracy01=0.524, loss=0.675]\n",
            "[2020-05-11 12:49:26,447] \n",
            "7/10 * Epoch 7 (_base): lr=3.158e-06 | momentum=0.9000\n",
            "7/10 * Epoch 7 (train): accuracy01=0.5484 | loss=0.6967 | lr=3.677e-06 | momentum=0.9000\n",
            "7/10 * Epoch 7 (valid): accuracy01=0.5590 | loss=0.6972\n",
            "7/10 * Epoch 7 (test): accuracy01=0.5478 | loss=0.6893\n",
            "8/10 * Epoch (train): 100% 70/70 [00:02<00:00, 29.04it/s, accuracy01=0.600, loss=0.609, lr=2.105e-06, momentum=0.900]\n",
            "8/10 * Epoch (valid): 100% 9/9 [00:00<00:00, 78.00it/s, accuracy01=0.500, loss=0.703]\n",
            "8/10 * Epoch (test): 100% 9/9 [00:00<00:00, 76.28it/s, accuracy01=0.619, loss=0.676]\n",
            "[2020-05-11 12:49:29,490] \n",
            "8/10 * Epoch 8 (_base): lr=2.105e-06 | momentum=0.9000\n",
            "8/10 * Epoch 8 (train): accuracy01=0.5358 | loss=0.7018 | lr=2.624e-06 | momentum=0.9000\n",
            "8/10 * Epoch 8 (valid): accuracy01=0.5521 | loss=0.6923\n",
            "8/10 * Epoch 8 (test): accuracy01=0.5549 | loss=0.6885\n",
            "9/10 * Epoch (train): 100% 70/70 [00:02<00:00, 28.49it/s, accuracy01=0.300, loss=1.003, lr=1.053e-06, momentum=0.900]\n",
            "9/10 * Epoch (valid): 100% 9/9 [00:00<00:00, 81.29it/s, accuracy01=0.438, loss=0.738]\n",
            "9/10 * Epoch (test): 100% 9/9 [00:00<00:00, 70.81it/s, accuracy01=0.619, loss=0.675]\n",
            "[2020-05-11 12:49:32,410] \n",
            "9/10 * Epoch 9 (_base): lr=1.053e-06 | momentum=0.9000\n",
            "9/10 * Epoch 9 (train): accuracy01=0.5512 | loss=0.6958 | lr=1.571e-06 | momentum=0.9000\n",
            "9/10 * Epoch 9 (valid): accuracy01=0.5521 | loss=0.6942\n",
            "9/10 * Epoch 9 (test): accuracy01=0.5549 | loss=0.6881\n",
            "10/10 * Epoch (train): 100% 70/70 [00:02<00:00, 28.86it/s, accuracy01=0.500, loss=0.737, lr=0.000e+00, momentum=0.900]\n",
            "10/10 * Epoch (valid): 100% 9/9 [00:00<00:00, 76.95it/s, accuracy01=0.375, loss=0.757]\n",
            "10/10 * Epoch (test): 100% 9/9 [00:00<00:00, 79.47it/s, accuracy01=0.571, loss=0.675]\n",
            "[2020-05-11 12:49:35,401] \n",
            "10/10 * Epoch 10 (_base): lr=0.000e+00 | momentum=0.9000\n",
            "10/10 * Epoch 10 (train): accuracy01=0.5589 | loss=0.6984 | lr=5.188e-07 | momentum=0.9000\n",
            "10/10 * Epoch 10 (valid): accuracy01=0.5486 | loss=0.6955\n",
            "10/10 * Epoch 10 (test): accuracy01=0.5531 | loss=0.6879\n",
            "Top best models:\n",
            "logs/checkpoints/train.4.pth\t0.6878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgjuVS6CgYwA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c679c44-3b6e-4565-f4cc-5b272968e2ee"
      },
      "source": [
        "!ls ./logs/\n",
        "# y_pred = []\n",
        "# for batch in test_dataloader:\n",
        "#     logits = runner.predict_batch(batch)\n",
        "#     y_pred.append(logits['logits'].cpu().argmax(axis=1).tolist()[0])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_base_log  checkpoints\tlog.txt  test_log  train_log  valid_log\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViZwMS2pIkBv",
        "colab_type": "text"
      },
      "source": [
        "Ииии отчет!\n",
        "\n",
        "Напиши внизу небольшой отчет о проделанной работе. Ожидается сравнение результатов модели с разным количеством голов/слоев на разных датасетах на `test`. Если для оценки качества на датасете используется необычная метрика(не Accuracy или F1), то можно использовать один из них. Было бы круто, если бы вычислялась нужная метрика и она использовалась в отчете.\n",
        "\n",
        "<ТВОЙ ОТЧЕТ>\n",
        "\n",
        "</ТВОЙ ОТЧЕТ>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3PbkgpXbd3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "{epochs: range(1, 11),\n",
        " 'google/bert_uncased_L-2_H-128_A-2': {'loss': [],\n",
        "                                       'accuracy': []},\n",
        " 'google/bert_uncased_L-2_H-256_A-4',\n",
        "# 'google/bert_uncased_L-4_H-128_A-2', 'google/bert_uncased_L-4_H-256_A-4"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}